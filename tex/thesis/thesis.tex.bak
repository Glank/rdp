\documentclass[11pt]{article}
\usepackage{url}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{dirtree}
\usepackage{listings}
\graphicspath{{../../}}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{notation}{Notation}
\newtheorem{note}{Note}


\begin{document}


\title{Querying Semantic Data with Natural Language}
\author{Ernest Kirstein}
\maketitle
\clearpage

\tableofcontents
\clearpage

\begin{abstract}
Searching through semantic data using SPARQL requires expert knowledge and thus remains out of reach for casual users. In this work, we explore the development of natural language interfaces for semantic data. This work looks at a number of approaches towards this goal as well as providing a new approach. Our system works like a natural language compiler and handles a limited set of natural language questions which, though narrow in scope, have complex semantics that other systems would be unable to handle.
\end{abstract}
\clearpage

\section{Introduction}
The semantic web, as envisioned by Tim Berners-Lee et al., is an extension of
the internet which provides more structure to the vast chaos of data on the
net. The goal has always been for the semantic web to become a means for 
artificially intelligent agents to share information and reasoning\cite{semantic}.
Unfortunately for us humans, the semantic web has proven rather difficult for
casual users to interact with.

To unlock the power behind the semantic web, there needs to be a way for people
to interface with it and ask questions about stored (or reasoned) information. 
SPARQL is a common query language for searching through data on parts the 
semantic web\cite{sparql}. The main benefit using a formal query language 
is that it allows users to ask complex, exact questions. 
The main drawback to formal query languages is that
they have a steep learning curve and are really only viable for expert users.

So the usefulness of semantic data is largely inaccessible to casual internet users,
but it does not have to be. Natural languages (English, Spanish, etc.) are more than
capable of expressing the same sorts of questions that SPARQL can ask. But parsing and 
interpreting natural language is a little bit beyond
our current technology. Tools like Siri\cite{siri} have already begun to bridge that
gap but there is a lot of room for improvement. 

This thesis will discuss the various approaches that have been made to make
semantic data accessible through natural language interfaces, the direction of ongoing
research in the field, and our own contributions to the solution.

\subsection{The Semantic Web}

The definition of the semantic web is somewhat nebulous. Any semantic
data or interface for that data might qualify as part of the semantic web.
Semantic data is information which is structured in ways that allow artificial
intelligence agents to understand the meaning behind the data - data which
captures relationships and connections and not just numbers. 

The most common type of semantic data is in the form of `triples`. These are
small pieces of information which relate a subject and object via a predicate.
Triples store information like, ``John is an author." In such a sentence,
`John' would be the subject, `is a' the predicate, and `author' the object.
Except, in the semantic web, each of those concepts would be represented
by a URI (uniform resource identifier). So the triple would actually be
represented by something like, ``http://example.com/john rdf:type dbpdia-owl:writer''.

More complex information can be represented by multiple triples. As one might
imagine, this makes expressing information in semantic data a little tedious.
So, more commonly, semantic data is stored in specialized documents which 
translate into triples.

RDF\cite{rdf} is a semantic data framework which specifies some foundational URIs for triples
as well as an XML format to more easily express those triples. This framework
is used to describe object types (classes), properties of those types, properties
of those properties, and further foundation information for a well-rounded
grounding of more comprehensive semantic data. 
RDF documents are quite expressive in and of themselves.
Some basic reasoning engines can be built just rooted in RDF statements 
which propagate information about classes and their properties. For example, 
``Dogs bark. Spot is a dog. Can spot bounce?"

OWL\cite{owl} is an extension to the RDF framework which provides another layer
of reasoning to the rigid class specifications of plain RDF. For example,
"If x is a pet and x barks, x is a dog. Spot is a pet. Spot barks. Is spot a dog?"
RDF may make the semantic web expressive, but OWL allows it to be intelligent.

Semantic data is hosted on the internet and this makes up a part of the semantic web.
Interfaces to that data may are also a part of the web and many of these interfaces
use SPARQL. As mentioned before, SPARQL is a structured query language for asking
rigorously define questions over a semantic data set. SPARQL queries set up constrains
for triples or sets of triples to fit and an interface which executes SPARQL queries
returns the values from triples which match those constrains. Some engines also
incorporate reasoning using ontological and semantic knowledge - others simply look
at existing triples and return `dumb' results.

\subsection{DBPedia}

The website DBPedia\cite{dbpedia,dbpedia-swj} is a RDF data store containing information 
which has been curated from Wikipedia. It is a massive collection of structured data
containing over four million ontology-classified objects in over one hundred different
languages. It is one of the largest open-source RDF databases on the web.

The SPARQL queries in this work will target a portion of DBPedia's
data pertaining to books and authors. There is no clear division between this set of
data and the rest of the data store but that is just one more real-world complexity
to hurdle. Any system interacting with RDF data should consider
the data's naturally unbounded scope, even if the system itself is more constrained.

DBPedia has its own ontology and ways of relating objects to one another. This means
that, although the general principals involved would be the same, 
it may be more difficult to create an interface for another data set than simply 
replacing the URIs from DBPedia with another database's URIs. This `portability problem'
is an issue that is an area of active research\cite{issues, usability}, 
but is not a key focus of this work.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth,natwidth=1,natheight=1]{umlet/book_ontology.pdf}
    \caption{A few classes of the DBPedia ontology}
    \label{fig:book_ontology}
\end{figure}

Figure \ref{fig:book_ontology} shows a small portion of the DBPedia ontology that
will be used in this work. The RDFS labels of each class are what plug into the
natural language parser to recognize the names of books, authors, and genres. 
The classes are related by DBPedia's own property predicates.

Three classes of objects are enough to ask challenging questions but keep the
scope within range of a research project. We hoped our system would be able to 
handle questions as complex as:
\begin{itemize}
\item What books has Heinlein written?
\item Who wrote Fahrenheit 450?
\item Which authors have written both science fiction and high fantasy novels?
\item Which books has Dave Wolverton written that are the same genre as Game of Thrones?
\end{itemize}

\section{Previous Work}

Work by Kaufmann and Bernstein \cite{usability} indicates
that users have a ``clear preference" for even limited natural language interfaces
when compared to keyword or query language interfaces.
Casual users of the semantic web have needs that require a more precise interface
than the common keyword-based search engines of the early web. But they also
need a less rigorous tool than a formal query language. \cite{usability}

Other recent works \cite{mapping, freya, galitsky, nlp-reduce, galitsky2, querix}
have shown that simple natural language questions can be translated based
on known sentence structure or leveraging named-entity recognition, NER.

\subsection{Other NLIs}
Many natural language interfaces, NLIs, have been developed since they gained traction
in the 70s\cite{usability}. This section will discuss a few of the more recent ones.

NLP-Reduce\cite{usability,nlp-reduce} is a highly portable system which supports full
natural language questions, fragment questions, and keyword questions.
It leverages relationships between semantic classes while deliberately avoiding
``complex linguistic and semantic technologies"\cite{nlp-reduce}.
Querix\cite{querix} is a more restricted NLI by some of the same developers as
NLP-Reduce. It only handles specific question beginnings and specific question forms. 
This interface is more limited but was shown to be preferred buy casual 
users\cite{usability}.

Another earlier system by Glaitsky\cite{galitsky2} focused more on the logical complexity
of some questions. His proposed approach used `semantic headers' to reason out
complex questions that involved making `commonsense' deductions.

FREyA\cite{freya} is a system which focused on the portability problem of most
NLIs. This system applied Stanford's part-of-speech parser and custom heuristic algorithms
to classify groups of words into potential ontology concepts. Comparing an OWL ontology
with the potential ontology concepts of the heuristic output, they were able to generate
SPARQL queries. The system even applied some machine learning techniques in the 
heuristic algorithm to achieve marginal improvements to question recognition and SPARQL
generation. 

\subsection{Common Issues}
A recent publication by Sharef et al. \cite{issues} outlines obstacles in developing full
natural language interfaces for the semantic web. That paper notes a particular
difficulty with parsing what we call `multifaceted' questions - questions with multiple
variables, constraints, or operations. This is the precise gap which this work
attempts to bridge.

NLIs for the semantic web will also eventually need to deal with multiple sources
and unmitigated ontologies that need to be unified to have truly global search
capabilities\cite{issues}. This might even include issues of trusting information
sources\cite{semantic}.

Linguistic variability and ambiguity make building such a
system a highly complicated and time consuming task. Also,
domain-restricted NLIs are difficult to adapt and port:
there is a trade off between retrieval performance and portability. 
Still, the meta-data of the semantic web provides assistance
which can potentially overcome some of these problems. \cite{usability}

Expressive interfaces, even in natural language, can be overwhelming.
As such, even NLIs require some user support and training (though much less than
a formal query language would). \cite{usability}

\section{Converting Natural Language Questions into \\SPARQL Queries}
Since SPARQL is already an established tool for navigating the semantic
web, it makes solid foundation to build higher level natural language queries.
This system we have built receives natural language questions such as, 
``What books has Dave Wolverton written that are the same genre as Game of Thrones?''
That question is parsed, related back to a custom indexing of DBPedia URIs, then
a SPARQL query gets compiled and executed.

In short, our approach was to build a `natural language compiler' of sorts. Something
robust enough to handle real-world natural language usage but rigid enough to translate
into SPARQL queries. So our research touches a little more on compiler design than 
one would typically see in natural language processing research.
The two areas of research have significant 
overlap\cite{chomsky, reghizzi} and other work has suggested that that a middle ground 
between these two research angles might be where NLIs find the most success\cite{usability}.


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth,natwidth=1,natheight=1]{umlet/usage.pdf}
    \caption{An overview of the NL to SPARQL Process}
    \label{fig:process}
\end{figure}

Figure \ref{fig:process} shows a brief outline of the full process. Questions come
in the form of natural language questions and are fed into a parser. The parser is
informed by the named entity recognizer which provides recognition of nouns, adjectives,
or verbs whose names must be scraped from the RDF database. The parser also 
leverages the python natural language toolkit\cite{nltk} for access to synonyms and
hyponyms. The parse tree (the output of the parser) then goes through a name/URI 
resolution process which matches the natural language names to their corresponding
RDF URIs. Finally, the resolved parse tree goes into a SPARQL-generating process 
which outputs a query.

\subsection{Parsing}

This system uses a custom parser which has been implemented especially for
this project. One of the contributions of this work is the unique implementation
of this parser. Parsing is typically done around a `grammar' which specifies how
words fit together and what is a valid statement within the context of a language.
There are other models for the description of a language\cite{chomsky} but
typically specifying a grammar like in figure \ref{fig:grammar} is one critical
aspect to developing a language-processing system.

Certain parsers require their grammars to have certain properties. Those properties
might be something of a hindrance to the development process, so we have sought
a way to work round the problem. Our parser automatically changes a grammar
to remove problems like left recursion and a few other hiccups which usually 
give top-down parsers trouble. But then the parser outputs
parse trees under the original structure of the input grammar.
This is accomplished by reversing the transformation required to remove left
recursion (etc.) using the algorithms described in section \ref{gram_transforms}.
This achieves a big design goal - it is a huge step towards decoupling the
design of the grammar from the rest of the system.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth,natwidth=1,natheight=1]{umlet/high_level.pdf}
    \caption{A high level overview of the parsing process.}
    \label{fig:high_level_parse}
\end{figure}

The grammar is manually coded to handle a small set of natural language questions
but is given more coverage by recognition of hyponyms and misspellings of named entities.
The specification of the grammar is a design task which could be partially automated by
using the ontology to enforce grammatical relationships between named entities. That
was a bit beyond the scope of this project but it is an idea to keep in mind for further
research.

\begin{figure}[h!]

\setcounter{equation}{0}
\begin{align}
S \rightarrow\; & subject\_question \label{gram:subj}\\
subject\_question \rightarrow\; & books\_request \label{gram:books}\\
subject\_question \rightarrow\; & authors\_request \label{gram:auths}\\
books\_request \rightarrow\; &
\text{WHAT} \; general\_books \; \text{HAS} \; \text{author} \; \text{WROTE} 
\label{gram:sim_books}\\
books\_request \rightarrow\; & \label{gram:comp_books}
\text{WHAT} \; \text{BOOKS} \; \text{HAS} \; \text{author} \; \text{WROTE}\\
&\text{THAT} \; book\_info \nonumber\\
authors\_request \rightarrow\; &
\text{WHAT} \; general\_authors \; \text{HAS} \; \text{WROTE} \\
&general\_books \nonumber \label{gram:comp_auths}\\
authors\_request \rightarrow\; &
\text{WHO} \; \text{HAS} \; \text{WROTE} \; \text{book}\\
authors\_request \rightarrow\; &
\text{WHAT} \; \text{AUTHOR} \; \text{WROTE} \; \text{book}\\
authors\_request \rightarrow\; &
\text{WHO} \; \text{WROTE} \; \text{book} \label{gram:who_wrote}\\
general\_books \rightarrow\; & \text{BOOKS} \label{gram:simp_gen_books}\\
general\_books \rightarrow\; & \text{genre}\\
general\_books \rightarrow\; & \text{genre} \; \text{BOOKS} \label{gram:comp_gen_books}\\
general\_books \rightarrow\; & 
\text{genre} \; \text{AND} \; \text{genre}\\
general\_books \rightarrow\; & 
\text{genre} \; \text{AND} \; \text{genre} \; \text{BOOKS}\\
general\_authors \rightarrow\; & \text{AUTHOR} \label{gram:simp_gen_auths}\\
general\_authors \rightarrow\; & \text{genre} \; \text{AUTHOR} \label{gram:comp_gen_auths}\\
book\_info \rightarrow\; & \text{ARE} \; \text{THE} \; \text{SAME} \; \text{GENRE} \;
\text{AS} \; \text{book} \label{gram:book_info}
\end{align}

\caption{Grammar}
\label{fig:grammar}
\end{figure}

Figure \ref{fig:grammar} describes the grammar which we used in our system to recognize
a limited set of natural language sentences. The grammar is composed of `production
rules' which describe how a string can be formed by replacing symbols within a
string. Strings always start off as a starting symbol, in this case it is ``$S$". From 
there, production rules are applied until a valid string is formed. If we follow rules 
\ref{gram:subj}, \ref{gram:auths}, and \ref{gram:who_wrote} we get the strings 
``$subject\_question$", ``$authors\_request$", then ``WHO WROTE book". This is a sentence
constructed from abstract `terminal nodes' which fit to a standardized word or words.
In this grammar we used upper case words to represent words or sets of words that are 
not associated with RDF entities. Lower case words represent RDF entities. 
So our constructed sentence might match the real question, ``Who wrote The Giver?"

The terminal nodes for some words also leverage the python NLTK\cite{nltk} to gain access 
to synonyms, hyponyms, and different tenses/pluralities of the word; 
for instance `WROTE' actually matches a lot of words; 
`written', `penned', `authored', `scrawled'. Other terminal nodes
simply include a fixed set of words that were manually entered.

For another example, if we followed the rules \ref{gram:subj}, \ref{gram:auths}, 
\ref{gram:comp_auths} then our strings would progress as follows: 
``$S$", ``$subject\_question$", ``$authors\_request$", 
``WHAT $general\_authors$ HAS WROTE $general\_books$". Rule \ref{gram:subj} told us to 
replace $S$ with $subject\_question$. Rule \ref{gram:subj} told us to replace
$subject\_questions$ with $authors\_request$. Nodes that get replaced like this
are called `non-terminal'. We can apply two more rules for each of our remaining
non-terminals: $general\_authors$ and $general\_books$. If we chose the rules
\ref{gram:comp_gen_auths} and \ref{gram:comp_gen_books} then we end up with the
string ``WHAT genre AUTHOR HAS WROTE genre BOOKS". This question might match the
real natural language question, ``What science fiction author has written 
high fantasy novels?" Notice that, in the grammar, non-terminal nodes are
in italics.

Grammar rules are specified for each type of question and each descriptive sentence
fragment which corresponds to some SPARQL generation abstraction (see 
figure \ref{fig:grammar}). In our example grammar, you'll see `general books' as a
non-terminal grammar node which corresponds to phrases like ``science fiction novels",
``history and fantasy books", etc. Choosing how to design the grammar is not an
easy task and required several iterations before some design principals became apparent.
For more on this, see section \ref{gen}. 

\begin{figure}[h!]
\dirtree{%
.1 $S$.
.2 $subject\_questions$.
.3 $books\_request$.
.4 WHAT.
.4 BOOKS.
.4 HAS.
.4 author: Dave Wolverton, 0.408655.
.4 WROTE.
.4 THAT.
.4 $book\_info$.
.5 ARE.
.5 THE.
.5 SAME.
.5 GENRE.
.5 AS.
.5 book: Game of Thrones, 0.928662.
}
\caption{Parse Tree - the Output of the parser for the question, 
``Which books has Dave Wolverton written that are the same genre as Game of Thrones?"}
\label{fig:parse}
\end{figure}

A parse tree is a hierarchical representation of the application of a grammar's rules.
For example, the parse tree in \ref{fig:parse} shows the how the application of rules
\ref{gram:subj}, \ref{gram:books}, \ref{gram:comp_books}, and \ref{gram:book_info}
are applied to build the string, 
``WHAT BOOKS HAS author WROTE THAT ARE THE SAME GENRE AS book". The parse takes
natural language sentences like, ``Which books has Dave Wolverton written that
are the same genre as Game of Thrones?" and determines the appropriate parse tree which
corresponds to it.

Given a natural language question, the parser actually produces a number of
parse trees. Each parse tree can be considered an interpretation of the question,
so the parse then selects the `best' (most probable) interpretation. The probability
of the interpretation depends on how close the named entities found in 
the interpretations match the model defined for that entity (see section \ref{ner}). 
The probability of the interpretation is estimated by taking the product of the 
estimated probabilities of all the named entities (again, see section \ref{ner} for
more details). The interpretation goes through the query generation process to see if 
there are any logic conflicts that the grammar cannot see. If an error is thrown generating the
SPARQL, the next best interpretation is used.

The example question's parse tree in figure \ref{fig:parse} shows the parse trees nodes
in a tabbed hierarchy. On the left are the names of the nodes and to
the right of each name you'll see some representation from the sentence that 
corresponds with the node. Named entity nodes also have a number which is the
`information content' of that node (an inverse log of the probability) which is higher
the further away a string is from the expect average in that named entity class.
This value is explained in more detail in section \ref{ner}.

\subsection{Named Entity Recognition}
\label{ner}
Natural language names of `named objects' are recognized by a component of the system
called the `named entity recognizer' \cite{ner1,ner2}. 
The NER would be rather poor at prediting the type associated with a given name if that
were the only information it had to go on. That is why each named entity type is associated
with a terminal node in the grammar - it gains a great deal of accuracy by narrowing down
the possible type for a given string using the context the grammar provides.
Setting up the NER requires a bit of preprocessing:
lists of natural language names are downloaded from the semantic database using
manually entered SPARQL queries then compiled into classifier models.

Typically, NERs use some means of machine learning to classify objects within a
sentence based on parts of speech, word or phrase feature extraction, and other
rather advanced topics\cite{ner1}. 
For our work, the definition of classes for our named entities was determined by the
DBPedia ontology. Also, having access to the data, we also have a rather large sample
set of names for each classification.

So for each class we built bloom filters\cite{bloom} and n-gram classification models 
which could give a simple probability measure of, ``Is X string Y classification?". 
The n-gram classification models were rather simple to implement and worked well 
with the Jaccard index comparison we use in section \ref{resolution}. We simply
checked for the absence or presence of n-grams as features and use a naive
Bayesian classifier over that feature set\cite{bayes} so the same features
informed both the NER and the resolution process.

Bayesian classification involves calculating the probability of feature results
(in this case the absence or presence of an n-gram in a string) then using the product
of all those probabilities as an estimation of the total probability of that string
occurring in a given set assuming that all features are independent. This is a
pretty bold assumption and doesn't generally hold true for an n-gram feature set,
but it gives a rough estimation which is all that was necessary in this case.

Our n-gram classification models capture features like common prefixes and suffixes
while bloom filters trained on a range of single-edit variations of a name help
to catch non-standard names. Other more accurate models exist, but these served
our purposes and had certain properties that were important within the context
of the parser.

These models need to allow for probabilistic recognition of the input strings in 
below $O(n)$ time (in relation to the number of objects in the RDF
database) to be done efficiently within the context of the parser. 
Additionally, they need to recognize variations on
the names (typos, variations, etc.) within some user-defined as a standard feature
that casual users expect from a search engine.

Furthermore, these models need to provide faster than $O(n)$
approximation of their 'probability' within the set of all the names of instances
of that class. This is important for determining which interpretation of a
question is the correct one: we can generally assume that the most probable
interpretation is the correct one. Exceptions of that rule are where there exist
logical ambiguities rather than semantic ones (``How big is New York?" is a common example
of an ambiguous question - which New York and how is `big' defined?). 

This probability is expressed as `information content'\cite{shannon} which is
the value that you'll see in figure \ref{fig:parse}. 
Information content is a function which is useful for expressing very small 
probabilities, there was no other significant reason for expressing
probabilities in this form. Suffice it to say that larger values for information content
mean smaller probabilities and ranges from zero to positive infinity. 
Where $I(x)$ is the information content in an event
$x$ and $P(x)$ is the probability:

\[ I(x) = -\log(P(x)) \]

\subsection{Resolution}
\label{resolution}
After parsing, specific instances of named entities need to be `resolved' to
corresponding RDF entities. Natural language names (like "John Smith") are mapped
to RDF URIs (like ``http://sbc.net/smith394")
in a semi-automated process. This boils down to searching a
database of names with a good fuzzy string matching algorithm and falling
back on the user to select the appropriate name when there is no obvious best 
match.

The resolution process differs from NER name matching in that the NER 
only determines whether or not a name belongs to a certain type. For instance, the NER
determines if `John Smith' is an author, but the resolution process will match that
name to a URI like `http://example.org/john\_smith'. The NER can determine weather
a name belongs to a set quicker than the resolution process can determine exactly
which URI matches a name.

A lot of work went into selecting the best fuzzy matching algorithm for
natural language names. Though Levenshtein's `edit distance' metric
is a good comparison function for spelling mistakes, we've found that
a Jaccard index based function is more suited to the task. 

\begin{figure}[h!]
\dirtree{%
.1 $S$.
.2 $subject\_questions$.
.3 $books\_request$.
.4 WHAT.
.4 $general\_books$.
.5 genre: science fiction, 4.602974.
.5 BOOKS.
.4 HAS.
.4 author: {E. A. Poe}, 2.184804.
.4 WROTE.
}
\caption{The parse tree for the question, 
``What science fiction books has E. A. Poe written?"
There are two named entities in this parse tree, the genre and the author.}
\label{fig:resolve}
\end{figure}

Figure \ref{fig:resolve} shows a case where semi-automatic reasoning is applied.
For the first named entity, `science fiction' is automatically paired with the
named entity `dbp:Science\_fiction' because its RDF label, `Science Fiction', is 
considered an exact match using our string comparison function. 
Notice that it is a relatively poor named-entity match within the statistical model
used for parsing since it has an information content of 4.6. 
The name `E. A. Poe' is another mater. It is
very close to two author names using the Jaccard index comparison function; both
`Edgar Alen Poe' and `Michael Poe' are equally close matches by this metric so the
user is prompted to choose between them.

\subsection{Generation}
\label{gen}
The last step is to actually produce SPARQL queries from the resolved parse trees.
Being the last step in a complex process, it is tightly coupled to the output of the
previous steps which means that this part needs to be particularly adaptable to design
changes. Minor changes to the grammar, adding or removing named entity types, and
any otherwise insignificant change trickels down to cause problems in this step.
However, it is difficult to design this generation process due, in part, 
to the complexity of the SPARQL query language. 
So, it took several iterations to refine our approach which are described
in more detail in section \ref{section:sparql_gen}.


\begin{figure}[h!]
\dirtree{%
.1 $S$.
.2 $subject\_questions$.
.3 $books\_request$.
.4 WHAT.
.4 $general\_books$.
.5 genre: science fiction, 4.602974.
.5 BOOKS.
.4 HAS.
.4 author: {George Martin}, 0.488245.
.4 WROTE.
}
\begin{verbatim}
PREFIX dbpprop: <http://dbpedia.org/property/>
PREFIX dbp: <http://dbpedia.org/resource/>
PREFIX dbpowl: <http://dbpedia.org/ontology/>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
SELECT DISTINCT ?book ?title ?genre
WHERE {
    ?book rdfs:label ?title;
        dbpprop:genre ?genre;
        dbpowl:author dbp:George_R._R._Martin;
        dbpprop:genre dbp:Science_fiction.
    FILTER langMatches( lang(?title), "EN" ).
}
LIMIT 100
\end{verbatim}
\caption{An example of SPARQL being generated from a parse tree.}
\label{fig:gen_example}
\end{figure}

You can see in figure \ref{fig:gen_example} an example of the final output from the
SPARQL generation process. In this instance, three generation rules were applied to
the parse tree during the generation phase; one which setup the query for being
a DBPedia select query (configuring namespaces mostly); one which configured
the query to be a request for books (selecting outputs and specifying key variables);
and the last to link the subject variable to the general books adjectives (specifying
the books grammar). 

\clearpage

\section{Software Architecture}

This architecture is aimed at handling complex questions in a narrow domain.
It does more than named entity recognition - it actually considers
the full syntax of the language and processes natural language questions
much like a compiler might process source code. It uses
top-down parsing to generate a parse tree for the input question then
compiles SPARQL queries from that parse tree.
This section will explain the design of the system at the highest level.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth,natwidth=1,natheight=1]{umlet/architecture.pdf}
    \caption{Architecture}
    \label{fig:arch}
\end{figure}


One of the goals of this project was to develop a
highly extensible architecture. There wasn't enough time to cover the
breadth of questions one would hope for in a mature product - such being
the nature of research. But we felt it was important to create a practical
working system that could handle a wide range of questions if more
time was dedicated towards that end.

To make the system extensible, it was important to decouple the various
components as much as possible. Still - language processing is a naturally
interdependent process with many overlapping concerns. For instance,
parsing seems to be an independent problem from named entity recognition (NER),
but to achieve better parsing results it was necessary to integrate NER into the
parser so that semantic context could inform the NER. 

Another design concern was the accessibility of the SPARQL endpoint (an online
interface where a remote server handles SPARQL queries over an RDF database).
SPARQL endpoints may connect to a large number of RDF databases (each of which is
a potential point of failure) and our experience has shown that they tend
to go down more often than other online resources. It would be ideal if the 
endpoint was always accessible, would accept an unlimited
number of queries, and would always respond quickly. But in practice, none of
those ideals are true. As such, the results of certain queries
(necessary for developing NER models and URI resolution) are cached into local files.
The trade-off is that data in the cache can stagnate. That problem is mitigated by 
simply flushing the cached results periodically.

\subsection{SPARQL Generation}
\label{section:sparql_gen}
SPARQL is a complex query language with a long, well-documented 
technical specification\cite{sparql, sparql11}. Generating SPARQL queries can be
more complicated than simply fitting URIs into a templated string which was our
first approach to the problem. In fact, it took us several iterations to realize
some of the underlying design goals when designing the SPARQL generation framework.

The first (inelegant) solution was to create a separate query-generating
classes that would handle each type of question on a case-by-case basis.
These classes did little more than fill in templated strings with URIs and did
nothing to capture the malleability of natural language.
Trying to extend the types of questions using that architecture, we found that
this approach wasn't feasible for a larger application.
Output from this approach was ugly but workable for simple questions (see the
appendix for an example of this old output).

In the next iteration, we improved the adaptability of the process by 
decoupling the query specification from the string generation. 
That is, we built a module to encapsulate the structure and output of SPARQL
queries with functions for common query-building tasks (adding triples, 
applying filters, etc.). Figure \ref{fig:sparql_gen} shows the design that came from
actually reading through the dense technical documentation and understanding the
underlying grammar of the SPARQL language.

In that iteration, it took many more lines of code to specify the queries since
we were explicitly constructing queries rather than relying on manually
entered strings. But at the same time, it became less error-prone, more
easily readable, and produced cleaner output.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth,natwidth=1,natheight=1]{umlet/sparql.pdf}
    \caption{SPARQL Generation \cite{sparql, sparql11}}
    \label{fig:sparql_gen}
\end{figure}

Query generation is a source of bugs that can be prevented by a well
designed framework. Every manually entered string is a potential source of bugs, the
architecture should reduce the need for manually inputting strings. Since SPARQL is
complex in and of itself, these manually entered strings are also a potential source
of logic errors. Though some logic errors are impossible to prevent using anything short of
a strong-AI, the proposed framework rigidly defines logical SPARQL code structures which
will aid in producing SPARQL queries with as few bugs as possible. 

Another reason to devlop a query-generating framework is that it helps to develop a more
modular system. Putting strings together is messy. Using a template system to
fill in URIs means that there is at least one step in the process where every part 
is coupled to every other part: the point where the values are injected into the template.
That makes it hard to design a modular system to interpret the parse trees which 
come out of the natural language parser. And it {\em will}
be necessary to subdivide that process in a larger, real-world system. With the building
encapsulated in a separate module, functions can be coded which more deftly manipulate
the query rather than just plugging in values. It will aid the development processes in
much the same way that automated-refactoring tools help coders produce code manually.

In the final iteration, we tried to improve the flexibility of the 
query-generating code by expressing the procedure in terms of re-usable
functions. Nodes from the parse tree are iterated over in a depth-first
algorithm which effects changes to the query at only key nodes such
as particular noun phrase nodes.

During that final iteration, it became clear that some grammars were much
easier to parse than others. Grammars which used typical grammar-school abstractions
for symbols (nouns, verbs, noun phrases, etc.) were easier to specify but were very
slow to parse (see figure \ref{fig:slow_grammar} in the appendix). Such grammars let in 
extraneous interpretations that (while grammatically
correct) bungle the named-entity-recognizer due to the number of possibilities which
need to be ruled out. For example, ``What scify books has john wane written?" is rather
easy to interpret because we can immediately classify `scify' as a genre and `john wane'
as an author's name.
It is easy to underestimate how much of that is from context; `scify' could be the
name of an author or `john wanee the name of a biography.

The NER's models are lax on purpose to account for these variabilities and
even names which might not be in the model's sample set: it will generally accept
strings as names more often than it will reject them so it has to check most words b
elonging to each type of noun (where the grammar allows). Then it needs to check many 
possible divisions between words and even needs to consider misspellings. All of this is handled more 
efficiently if one simply specifies the grammar to restrict the possible types 
within the grammar rather than relying on the NER completely. The grammar
in figure \ref{fig:grammar} is an example where this design principal was applied.


\section{Resolving User Input Names to RDF Entities}

Matching natural language names to RDF entities is essential to evaluating
natural language questions over RDF databases. This poses several problems:
names can be misspelled (e.g. "Swartseneger" for the label "Schwarzenegger"), 
may be reordered (e.g. "John Smith" for the label "Smith, John"),
or the name may be abbreviated (e.g. "R.L. Stine" for the label "Robert Lawrence Stine").
That's not to mention the small problem posed by people who have
changed their name entirely, sometimes multiple times
(e.g. "The artist formerly known as 'The artist formerly known as Prince'").
And despite all of these convolutions, a natural language system will need to
recognize and match these arbitrary instances with the often-sparse naming information
present in RDF data.

Our approach to this problem was to find a string distance function which was
robust to these changes and use that to simply find the best match name in $O(n)$ time 
(where $n$ is the number of names). Since the names are narrowed down by
context (using the parser), this was an acceptable solution: we don't need to iterate
over every single label in the RDF set, just those which belong to the particular
type of object the user is asking about.

There are a surplus of string comparison functions to choose from \cite{comparison}.
For our particular application, we have used a variation on the {\em Jaccard index}.
It is robust to misspellings and reorderings, with the added benefit of being quite efficient.
The Jaccard index is used in data mining for efficiently comparing long documents,
but it is comparable to other more complex methods of name comparison \cite{comparison} and 
our experience suggests it will work well here.

Another problem is resolving multiple close names (or even exactly the same name) to a single entity.
I took a simplistic approach (just asking the user) but we will also discuss other more
sophisticated possibilities for further research. 

\subsection{Name Standardization and Enumeration}
Before jumping into the specifics of the name comparison algorithm, there are a few issues
to deal with. Names with abbreviations, punctuation, and names with multiple parts can all
trip up comparison algorithms.

One standardization method we used was to remove punctuation and change all letters to upper case.
There may be a few edge cases where "John O'neal" isn't the same as "John Oneal",
but the mistake is acceptable the majority of the time. And such names are so close that they would
trigger the system to prompt the user for confirmation anyways.

Names with multiple parts, like "John Jacob Jingleheimer Schmidt", need to be matched by
partial variation like "Schmidt", "Mr. Schmidt", and "John Schmidt". 
Using a sufficiently robust string comparison function, these variations will often 
still match the full, multi-part name better than other multi-part names.
But that's a dubious assumption to rely upon - it is best to tokenize the name and
include the different partial variations as other names linked with the entity.
As part of our own system, we simply included the first and last names as variations on
the name, excluding the middle name(s).

\subsection{Jaccard Index}
The Jaccard index is a measure of how similar two sets are to each other.
This is useful in a whole host of applications \cite{general}, as you might imagine. 
In this application, using the Jaccard index on fragments of strings yields a very
robust string comparison function.

Let $A$ and $B$ be sets or multisets, then the Jaccard index $J(A,B)$ is defined \cite{mining, comparison}:
\begin{align*}
    J(A, B) = \frac{\left| A \cap B \right|}{\left| A \cup B \right|}
\end{align*}
Or if both sets are empty, $J(A,B) = 1$.
For comparing strings, the sets might be of characters (e.g. 'HELLO' $\mapsto \{E, H, L, O\}$), 
of tokens (e.g. 'JOHN H SMITH' $\mapsto \{\text{'JOHN'}, \text{'H'}, \text{'SMITH'}\}$),
or in this case, {\em n-grams}.

These n-grams (also known as 'k-grams' or 'shingles' \cite{mining}) are all unbroken substrings of 
length $n$ of a given string. So the 3-grams of the string 'anabasis' are 
$\{\text{'ana'}, \text{'nab'}, \text{'aba'}, \text{'bas'}, \text{'asi'}, \text{'sis'}\}$. 

In this application, n-grams were constructed to include imaginary pre and post string characters
(represented here as '\^{}' and '\$' respectively). So, for instance, the 3-grams of 'cat' are then
$\{\text{'\^{}\^{}c'}, \text{'\^{}ca'}, \text{'cat'}, \text{'at\$'}, \text{'t\$\$'}\}$. This gives
significance to the beginning and end of a string when using the Jaccard index with the n-grams.
We use enough prefix and suffix character so that n-grams start with or end with a single character
in the string; this gives a bit of additional weight and focus to prefixes and suffixes in the 
comparison function.

For a distance function, one can use:
\[d(s_1,s_2) = 1-J(ngrams(s_1),ngrams(s_2))\]
The Jaccard distance metric, $d$, is a metric space for ngrams \cite{data_mining}.
But it doesn't quite produce a true metric space for strings
(since two different strings can have exactly the same n-grams). Still, it does satisfy
the triangle inequality \cite{general} and is always non-negative. So one could
conceivably construct an M-tree \cite{mtree} (a datastructure which requires these properties)
to improve look-up speed. That hasn't been necessary in this research, but then again, 
this domain might be smaller than one might experience in practice.

\subsubsection{Comparison with Levenshtein (Edit) Distance}
Levenshtein distance (also known as 'edit distance') is a more common fuzzy string comparison algorithm
than the Jaccard index. 
Its ubiquity might be due to a simple happenstance: the algorithm for calculating edit distance is
a favorite example in algorithm design courses for demonstrating dynamic programming.
But its popularity is by no means a guarantee that it is the best choice, as I'll demonstrate.

Levenshtein distance is defined as \cite{levenshtein} the minimum number of 'edits' 
(additions, deletions, or swaps) that must occur before one string matches another.
But these are only single-character edits, and so the algorithm doesn't handle large displacements
of parts of the name with any sort of grace.
This is best demonstrated by example; see figure \ref{fig:lev_comp}.

\begin{figure}[h!]
\centering
Levenshtein Matching Results\\
\begin{tabular}{c|c|c}
  Distance & String A & String B \\
\hline
  14 & `global kirstein investing' & `kirstein global investing' \\
  13 & `global kirstein investing' & `scherl global investing' \\
  7 & `kirstein global investing' & `scherl global investing'
\end{tabular}
\\
Jaccard (3-gram) Matching Results\\
\begin{tabular}{c|c|c}
  Distance & String A & String B \\
\hline
  0.3125 & `global kirstein investing' & `kirstein global investing' \\
  0.5946 & `global kirstein investing' & `scherl global investing' \\
  0.5143 & `kirstein global investing' & `scherl global investing'
\end{tabular}
\caption{Bad Levenshtein Name Matching}
\label{fig:lev_comp}
\end{figure}

Using Levenshtein distance to match 'closest' strings, word 
inversions would be considered bulk deletions and insertions. 
The Jaccard index handles this better because inverting whole words 
still preserves the ngrams within those words even if it breaks
the joining ngrams between the words.

Admittedly, the Jaccard index is less forgiving of small typos. 
A single character edit breaks $n$ n-grams. So, for short single
word names, Levenshtein distance is probably the preferred metric.

\subsubsection{Information Content Sensitive Jaccard Index}
The Jaccard index by itself is a fairly good way to compare names.
But it would be better if the algorithm also noticed things like
how 'unusual' certain name patterns were. 
"Tom Smith" might be closer to "John Smith" than "Tom" based purely
on their Jaccard index, but any reasonable person would pick
"Tom Smith" and "Tom" to be the closer names because "Smith" isn't
the discriminating factor.

In more technical terms, the part of the string "Smith"
should receive less `weight' in the comparison function because
it conveys less information. 

Consider the more general form of the Jaccard index \cite{general}:
\begin{align*}
J(\vec{x},\vec{y}) = 
\frac{\sum_i \min(x_i, y_i)}{\sum_i \max(x_i, y_i)}
\end{align*}
Where $\vec{x}$ and $\vec{y}$ are large dimensional vectors of real numbers
rather than sets. To help relate it back to the original form, 
imagine that $\vec{x}$ and $\vec{y}$ are lists
of counts of all the possible things that could be in the two sets
(or multisets) $A$ and $B$.
\begin{align*}
x_i = count(e_i; A)\\
e_i \in \{A \cup B \}
\end{align*}

Where $y_i$ is defined similarly for $B$. Now if we want to consider the amount of information each little
portion of the string contains, we can calculate it
using the equation laid down by Shannon \cite{shannon}.
Namely, that the self information of an event is the log of the
inverse of the probability that event occurring. In this case,
the 'event' is the occurrence of a certain n-gram in the string.

\begin{align*}
I(e_i) &= \log\left(\frac{1}{P(e_i)}\right)\\
&= -\log(P(e_i))
\end{align*}

To determine this probability, we can look at the frequency
of the n-gram in a large sample set of names.
We need to index those names anyways, as part of the named entity
recognition process. With a large set, we can get pretty close to a true 
approximation of the probability of an n-gram occurring in 
a general population of those names:

\begin{align*}
P(e_i) \approx \frac{count(e_i; N)+1}{|N|+2}
\end{align*}

Where $N$ is the multiset containing the union of all the n-grams of
all the names. 
Then, we can weigh the vectors $x$ and $y$ such that we have
an information sensitive comparison function: $\hat{J}(x,y)$:

\begin{align*}
\hat{J}(\vec{x},\vec{y}) 
&= \frac{
    \sum_i I(e_i)\min(x_i, y_i)
}{
    \sum_i I(e_i)\max(x_i, y_i)
}\\
&= \frac{
    \sum_i \min(x_iI(e_i), y_iI(e_i))
}{
    \sum_i \max(x_iI(e_i), y_iI(e_i))
}\\
&= J\left(\vec{x} \cdot \vec{I}, \vec{y} \cdot \vec{I} \right)\\
\vec{I} &= \left< I(e_1), I(e_2), ... I(e_n) \right>
\end{align*}

\begin{figure}[h!]
    \centering
Matching Results without Information Content\\
\begin{tabular}{c|c|c}
  Distance & String A & String B \\
\hline
  0.6471 & `tom smith' & `john smith' \\
  0.7692 & `tom smith' & `tom' \\
  1.0 & `john smith' & `tom'
\end{tabular}
\\
\begin{tabular}{ c}
Sample Set\\
\hline
`adam smith' \\
`bob smith' \\
`carl smith' \\
`dale jones' \\
`ernest kirstein'
\end{tabular}
\\
Matching Results with Information Content\\
\begin{tabular}{c|c|c}
  Distance & String A & String B \\
\hline
  0.7296 & `tom smith' & `john smith' \\
  0.7235 & `tom smith' & `tom' \\
  1.0 & `john smith' & `tom'
\end{tabular}
\caption{Information Sensitive Name Matching}
\label{fig:lev_comp}
\end{figure}

The above example shows this comparison in action. In the top comparison,
without considering information content, the closest pair of names is
"tom smith" and "john smith". But after indexing the sample names, we
can calculate comparisons that do consider the information content. Then
we can see that the closer names are "tom smith" and "tom".

\subsection{Ambiguity}
What should the system do when the user inputs a name which is close
to the names of several entities by the chosen name comparison
function? The easiest solution would be to simply ask the user which
of the possible entities they meant. But this isn't always a great
solution; what if you're asking about a cornucopia of names?
Sure, it might be out of scope for this particular system to
resolve questions such as "Which of the actors in 'my\_data\_file.txt'
have been in movies together?" But that's certainly within the
realm of possibilities for some future work.

One approach to this problem would be to create a separate
system for distinguishing the "right" name from a small(er) selection
of possible candidates. The aforementioned Jaccard index (or similar
distance metric) might be used to narrow down the problem space
to something manageable, then a much more sophisticated (yet slower)
system could choose the right name from the smaller set.

Since there are a number of string comparison algorithms, one could
(if time permitted) implement several of them and use a combined
metric to evaluate the names for fitness. For example,
a feed-forward neural net could be trained to recognize the 'right'
name based on a number of factors:
\begin{itemize}
\item String comparison functions (Jaccard, Levenshtein, etc.)
\item The self-information \cite{shannon} of the input and potential match names
\item The closeness of other potential match names
\item The prevalence of the potential match entity in the RDF data
\item The frequency of queries involving the potential match entity
\item Etc.
\end{itemize}

\section{Top-Down Parsing}
Historically, top-down parsers were been coded manually or else generated from 
a context free grammar specification. \cite{lewis, formal_langs} 
Coding a RDP (recursive decent parser) manually is tedious, error prone, and difficult to maintain.
Programmatically generating a top-down parser from a grammar
still isn't ideal - converting a context free grammar into
a top-down parsable form may not preserve the {\em strong equivalence} of the grammar.
Two grammars are weakly equivalent if they define the same language (set of possible strings).
Two grammars are strongly equivalent if they are weakly equivalent and there is a one to one
corresondance between their parse trees which match equivalent strings. \cite{reghizzi}
And as a result, parse trees generated by that RDP will not be in the same form
as they might appear in the initial (non-RD-parsable) grammar, which is often a more 
natural representation of the desired language \cite{compiler}.

In this work, we hope to describe a useful adaptation to top-down parsing
which addresses these problems. Our system compiles a context free grammar 
specification into a top-down parsable grammar.
A parser is generated from the top-down parsable grammar,
and used to parse a input streams. 
The resulting parse trees are then transformed back into equivalent
parse trees under the original grammar. (Figure \ref{fig:high_level_parse})

\subsection{Introduction to Top-Down Parsing without Syntax Diagrams}
\label{rd_wo_sd}
Introductory material by Dr. Lewis \cite{lewis} describes a recursive-descent parser (a type of top-down parser)
as a piece of software which takes a sentence and turns it into a parse tree by performing a 'depth-first' search. 
But a search of what? One might try to call it a search of the parse tree, but that's not exactly right.
The 'recursive descent' name comes from the way that a RDP traverses through a
syntax diagram of a context free grammar \cite{compiler}.

Recursive decent is just one form of top down parsing. The top-down parsing in this thesis
does not use syntax diagrams (in hindsight, this was a questionable design choice, 
but such is life). This section will describe how top-down parsing works without
using syntax diagrams.

Consider a context-free grammar with the following production rules:
\setcounter{equation}{0}
\begin{align}
S &\rightarrow a S\\
S &\rightarrow b S\\
S &\rightarrow \epsilon
\end{align}
And the following string which we will attempt to parse: "ab"

We know, right off the bat, that the start symbol $S$ will be the root of any parse tree created under this
grammar, by virtue of it being the start symbol. (Figure \ref{fig:rdp_0})

\begin{figure}[h!]
    \centering
    \includegraphics[natwidth=15,natheight=15]{umlet/rdp_0.pdf}
    \caption{Parse Tree 0}
    \label{fig:rdp_0}
\end{figure}

The next parse tree we should consider is the parse tree that is generated when we follow the first 
production rule. (Figure \ref{fig:rdp_1}) Notice that, in this instance, the parse tree does not conflict with
the string we are trying to parse - i.e. regardless of the production rules we follow the string that is
produced from any further production rules we follow will start with "a".

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth,natwidth=30,natheight=30]{umlet/rdp_1.pdf}
    \caption{Parse Tree 1 - Valid}
    \label{fig:rdp_1}
\end{figure}

For the next parse tree, we will try to repeat are last action (following the first possible production rule).
(Figure \ref{fig:rdp_2}) This parse tree conflicts with the string we are trying to produce since any
string produced by further production rule applications will produce a string starting with "aa".
In this case, we go back to the previous parse tree and try a different production rule.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth,natwidth=30,natheight=30]{umlet/rdp_2.pdf}
    \caption{Parse Tree 2 - Invalid}
    \label{fig:rdp_2}
\end{figure}

In figure \ref{fig:rdp_3}, we've followed the second production rule
and our new parse tree fits with the input string, so we can continue our descent.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth,natwidth=30,natheight=30]{umlet/rdp_3.pdf}
    \caption{Parse Tree 3 - Valid}
    \label{fig:rdp_3}
\end{figure}

The next two parse trees (Figure \ref{fig:rdp_4_5}), created by applying the first and second production rules to
Parse Tree 3, are both invalid because they extend past the length of our input string.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth,natwidth=30,natheight=30]{umlet/rdp_4.pdf}
    \includegraphics[width=0.4\textwidth,natwidth=30,natheight=30]{umlet/rdp_5.pdf}
    \caption{Parse Trees 4 and 5 - Both Invalid}
    \label{fig:rdp_4_5}
\end{figure}

In the last step, by applying the third production rule to Parse Tree 3, we have a parse tree which terminates and produces the
desired input string, "ab". (Figure \ref{fig:rdp_6})

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth,natwidth=30,natheight=30]{umlet/rdp_6.pdf}
    \caption{Parse Tree 6 - Complete}
    \label{fig:rdp_6}
\end{figure}

Finally, let's diagram our traversal through the possible parse trees (Figure \ref{fig:rdp_7}). Shown this way, one can notice
a pattern in our attempt to build the tree. The top-down parser performs a depth first search of the 
graph of possible parse trees, looking for a parse tree which fits the input string.
The child nodes from each PT (Parse Tree) node in the graph are PT nodes generated by
applying each of the production rules to the first (left, deepest) nonterminal symbol in that PT node.
The depth of each node in the PT tree corresponds with the number of production rules that have been applied.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth,natwidth=30,natheight=30]{umlet/rdp_7.pdf}
    \caption{Parse Tree Search Progression}
    \label{fig:rdp_7}
\end{figure}

\clearpage

\subsection{Compiling a Context Free Grammar for Top Down Parsing}

It is important to notice that not all context free grammars can be directly parsed by a
top-down parser. Some context free grammars require a bit of manipulation to remove
left recursion (direct or otherwise) \cite{compiler}. This process of converting a context
free grammar into a {\em weakly equivalent} top-down parsable grammar shall be referred to as
{\em compiling} the grammar.

A grammar is formally defined from an ordered collection of production rules.
Our parser uses context-free grammar rules, which are comprised of a
'head' (the single-symbol left hand side of the production rule), and a 'tail'
(one or more symbols comprising the right hand side of the production rule).

These grammars may be `compiled' using the four procedures:
factoring, substitution, removing left recursion, and removing useless
rules. Let `decision list' define an ordered list of production rule
choices which produces a parse tree.
As each of these four procedures changes an input grammar into a weakly equivalent 
output grammar, there exists a mapping for any decision list in a compiled grammar
back into a same-terminal-producing decision list in the pre-compiled (parent) grammar.
Our parser keeps track of these inverse transformation rules as performs
its compilation procedure so that a compiled grammar's decision list (the result of
parsing a string) can be easily converted to the initial grammar's equivalent decision list. 

Take this simple grammar for example:
\setcounter{equation}{0}
\begin{align}
S &\rightarrow A B\\
A &\rightarrow a\\
A &\rightarrow S A\\
B &\rightarrow b\\
B &\rightarrow S B
\end{align}
It compiles into the weakly equivalent grammar:
\setcounter{equation}{0}
\begin{align}
Z &\rightarrow \epsilon\\
B &\rightarrow b\\
S &\rightarrow a B S'\\
S' &\rightarrow \epsilon\\
A &\rightarrow a Z\\
B &\rightarrow a B S' B\\
S' &\rightarrow a Z B S'\\
Z &\rightarrow b S' A\\
Z &\rightarrow a B S' B S' A
\end{align}

So when the terminal stream "aabb" is parsed in the compiled
grammar to the decision list $[3, 6, 2, 4, 2, 4]$ it can be transformed
into the parent-grammar-equivalent decision list: $[1, 2, 5, 1, 2, 4, 4]$.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth,natwidth=458,natheight=444]{umlet/compiled_ex.pdf}
    \includegraphics[width=0.4\textwidth,natwidth=472,natheight=500]{umlet/decompiled_ex.pdf}
    \caption{Compiled Grammar Parse Tree (left) Parent Grammar Parse Tree (right)}
    \label{fig:comp_to_dec_ex}
\end{figure}

\subsection{Effect of Grammar Transformations on Parse Trees}
\label{gram_transforms}
Left recursion is a problem for top-down parsers because it may cause them to
go into an infinite loop. Using the model described in section \ref{rd_wo_sd}:
when a PT node is reach where the first nonterminal symbol has a production rule with
left recursion, its child node will have the same nonterminal symbol so it will produce
a child node with the same nonterminal symbol ad infinitum - and none of those children will
consume any terminals from the input stream so the parser will not proceed. 

So, removing left recursion from context free grammars is a necessary evil for top-down parsing.
It just takes two transformations to turn any context free grammar with left recursion
into a {\em weakly equivalent} grammar with only right recursion.
These two transformations are direct left recursion elimination, 
$DLRE(G; R_\alpha, R_\beta) \rightarrow G'$ and substitution, 
$SUB(G; R_\alpha, R_\beta) \rightarrow G'$, which is necessary to
remove indirect left recursion. \cite{aho, lewis}
These sections will describe how these transformations work on the grammar
and on their parse trees.

\subsubsection{Substitution}

A grammar, $G$, where substitution can be applied has rules for some non-terminals $A$ and $B$ of the form:
\begin{align*}
R_\alpha &= A \rightarrow B \alpha\\
R_\beta(i) &= B \rightarrow \beta_i
\end{align*}

Where $i \in [1,m]$. Let $R_\beta$ represent the sets of the $B$ rules where the notation
$R_\beta(1)$ represents $B \rightarrow \beta_1$. 
Such a grammar represents a language which contains substrings of the form:
\[ S = \beta_x \alpha\] 
With $x \in [1,m]$.
To produce such a substring, the rules need to be followed in the opposite order:
\[Prods(S;G) = \{R_\alpha, R_\beta(x)\}\]
Where $Prods$ is the function which outputs the production rules from $G$ which generate the substring $S$.

The transformation \cite{aho} $SUB(G; R_\alpha, R_\beta) \rightarrow G'$ which applies the substitution
replaces the single-element $R_\alpha$ rule set with combined rules for each $R_\beta$:

\[ R_\alpha(i)' = A \rightarrow \beta_i \alpha \]

Then, to produce the same substring $S$, just a single new $R_\alpha$ rule is followed. 
\[Prods(S;G') = \{R_\alpha(x)\}\]

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth,natwidth=1,natheight=1]{umlet/sub_orig.pdf}
    \includegraphics[width=0.4\textwidth,natwidth=1,natheight=1]{umlet/sub_comp.pdf}
    \caption{Original and Transformed Parse Trees}
    \label{fig:dlre}
\end{figure}

The transformation just removes the $B$ node in the parse tree (Figure \ref{fig:dlre}).
So the inverse transformation simply puts the $B$ node back in.

More formally, for each $A$ node in the transform-affected parse tree, the inverse transformation $SUB^{-1}(T'; A, B) \rightarrow T$
modifies trees rooted at $A$ nodes where the children of the $A$ node are just some $\beta_x$ and $\alpha$.
The $\beta_x$ node will be replaced with a $B$ node which then just connects back to the $\beta_x$ node.

\subsubsection{Direct Left Recursion Elimination}
A grammar with direct left recursion, $G$, has rules for some nonterminal $A$ of the form:
\begin{align*}
R_\alpha(i) &= A \rightarrow A \alpha_i \\
R_\beta(j) &= A \rightarrow \beta_j
\end{align*}
Where $i \in [1,n]$ and $j \in [1,m]$.
Let $R_\alpha$ and $R_\beta$ represent the sets of those rules respectively where the notation
$R_\alpha(1)$ represents $A \rightarrow A \alpha_1$. 
Such a grammar represents a language which contains substrings of the form:
\[ S = \beta_y \alpha_{x_1} \alpha_{x_2} ... \alpha_{x_p}...\alpha_{x_{k-1}} \alpha_{x_k}\] 
where $y \in [1,m]$ and each $x_p \in [1,n]$.
However, to produce such a substring, the $\alpha_*$ rules need to be followed in reverse order:
\[Prods(S;G) = \{R_\alpha(x_k), R_\alpha(x_{k-1}), ... R_\alpha(x_{p}), ... R_\alpha(x_2), R_\alpha(x_1), R_\beta(y)\}\]
Where $Prods$ is the function which outputs the production rules from $G$ which generate the substring $S$.

The transformation \cite{aho} $DLRE(G; R_\alpha, R_\beta) \rightarrow G'$ which achieves Direct
Left Recursion Elimination transforms each of those $R_\alpha$ and $R_\beta$ rules to corresponding 
$R_\alpha'$ and $R_\beta'$ rules respectively:
\begin{align*}
R_\alpha'(i) &= A' \rightarrow \alpha_i A'\\
R_\beta'(j) &= A \rightarrow \beta_j A'
\end{align*}
and adds an additional rule:
\[ R_\epsilon = A' \rightarrow \epsilon\]

Consequently, the order of the followed production rules in $G'$ to produce the same substring uses rules in forward order:

\[Prods(S;G') = \{R_\beta'(y), R_\alpha'(x_1), R_\alpha'(x_2), ... R_\alpha'(x_{k-1}), R_\alpha'(x_k), R_\epsilon\}\]

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth,natwidth=1,natheight=1]{umlet/dlre_orig.pdf}
    \includegraphics[width=0.4\textwidth,natwidth=1,natheight=1]{umlet/dlre_comp.pdf}
    \caption{Original and Transformed Parse Trees}
    \label{fig:dlre}
\end{figure}

The transformation effects the parse trees by flipping and reversing $A$ chains,
replacing lower $A$ nodes with $A'$ nodes, moving the $\beta$ up,
and adding an $A'$ node and $\epsilon$ node to the end of the chain. (Figure \ref{fig:dlre}) The inverse transformation
removes last $A'$ and $\epsilon$ nodes, moves the $\beta$ back down to the end of the chain, and changes the $A'$ nodes back into
$A$ nodes, then flips and reverses the $A$ chain. Note that the $\alpha$ and $\beta$ nodes are arbitrary substrings, so in reality
they might be multiple nodes which might have any number of children.

More formally, for each $A$ node in the transform-affected parse tree, the inverse transformation $DLRE^{-1}(T'; A, A') \rightarrow T$
modifies chains of $A/A'$ nodes where the children of the $A$ node are $C_A$, of all but the last $A'_{x_p}$ node are $C_{A'_{x p}}$,
and the last $A'_\epsilon$ node has only the child $\epsilon$.
From the $DLRE$ transformation rules we know that the $C_A$ will be of the form $\beta_y A'_{x_1}$. 
We can also conclude from the $DLRE$ transformation that each $C_{A'_{x p}}$ will be of the form $\alpha_p A'_{x_{p+1}}$
when $p < k$ and $A'_\epsilon$ when $p=k$. 

The inverse transformation first removes the $A'_\epsilon$ node. Then it changes each remaining $A'_{x_p}$ node into an $A_{x_p}$ node
with the same children. Next the $A_{x_p}$ are restructured such that each $C_{A_{x p}}$
is equal to $A_{x_{p-1}} \alpha_p$ where $1 < p$ and $\beta_y \alpha_k$ where $p=1$. The top node of the chain, 
$A$, is replaced with $A_{x_k}$. And this process is repeated for each chain.

\section{Future Work}
As this project draws to a close, there have been a number of key areas where more work
could have been done and where this work might lead. 
Recent works have directed their research towards the portability problem as a key area 
for research. The portability problem being: the ability to quickly generate or adapt 
a NLI to a particular semantic dataset or interface. 
One could say that, although this is obviously
an important area to push forward, we should also focus on creating interfaces with a
deeper understanding of their subject matter. NLIs which focus on niche subjects in great
detail may have more potential to attract casual users to the semantic web than broader
but less expressive NLIs which may only provide marginal benefits compared with the 
current internet.

\section{Conclusion}
The issue with natural language interfaces is the insurmountable scope of the problem. 
Natural language is too rich to ever be completely documented. Perhaps even more troubling,
natural language is a moving target; always evolving along with society and the times.
The lines between languages can be blurred. The lines between dialects even more so.
And that leads to the obvious question: how can we ever hope to get there?

We do have a fairly solid framework for understanding languages as static constructs
\cite{chomsky}. We're slowly accumulating knowledge in the semantic web like 
Tim Berners-Lee imagined, though not yet to the extent that he'd hoped. Perhaps it is
naive of me to think so, but this seems like the kind of problem where progress
will be exponential. When we finally do start making serious headway into creating
natural language interfaces for the semantic web, we'll be able to communicate and
accumulate knowledge more effectively which will increase the rate at which we'll be
able to improve natural language interfaces ad infinitum.

Even just in this work we've demonstrated how one could build an in-depth natural
language interface for a narrow domain - perhaps that narrow domain will be one of 
NLI development in later work.

\clearpage
\section*{Terms}

\begin{itemize}
\item \textbf{Grammar}: a phrase-structure grammar is defined by a finite vocabulary (alphabet), a finite set of
initial strings, and a finite set of rules... \cite{chomsky} (see Production Rule)
\item \textbf{Context-Free Grammar}: a context free grammar is one which only has production rules whose head is a single nonterminal symbol.
\cite{compiler, anatomy, formal_langs}
\item \textbf{Production, Production Rule, Rewrite Rule}: rules of the form $X \rightarrow Y$ where
$X$ and $Y$ are strings in a grammar (ordered lists of symbols of the grammar)  \cite{chomsky};
define the nonterminal symbols by sequences of terminals and nonterminal symbols \cite{compiler};
rules which specify how nonterminal symbols may be expanded into new sequences of symbols (terminal or otherwise).
\item \textbf{Head (Production Rule)}: the left hand side of a production rule
\item \textbf{Tail (Production Rule)}: the right hand side of a production rule
\item \textbf{Parse Tree}: an ordered, rooted tree whose nodes are symbols in a context-free grammar where the 
children of each branch node correspond to the tail of some production rule in said grammar;
a tree-representation of the grammatical structure of an input stream \cite{anatomy}
\item \textbf{Weakly Equivalent (Grammar)}: two grammars are weakly equivalent if they define the same language.\cite{reghizzi}
\item \textbf{Strongly/Structurally Equivalent (Grammar)}: two grammars are strongly equivalent if they are weakly equivalent and there is a one to one
corresondance between their parse trees which match equivalent strings. \cite{reghizzi}
 \cite{reghizzi}
\end{itemize}

\clearpage
\section*{Appendix}

This appendix contains miscellaneous figures and examples that came out of this work.

Below is an example of SPARQL generated by one of the early iterations of our generation process
which used simple string template filling. You'll notice some constants that are
not abbreviated where they could be and less standard whitespace.

\begin{verbatim}
PREFIX dbpprop:<http://dbpedia.org/property/>
PREFIX dbp:<http://dbpedia.org/resource/>
PREFIX dbpowl:<http://dbpedia.org/ontology/>
PREFIX rdfs:<http://www.w3.org/2000/01/rdf-schema#>
SELECT ?author ?name
WHERE{
    ?author a dbpowl:Writer;
        rdfs:lable ?name.

    ?book0 a bpowl:Book;
        dbpowl:author ?author;
        dbpprop:genre <http://dbpedia.org/resource/High_fantasy>.


    ?book1 a bpowl:Book;
        dbpowl:author ?author;
        dbpprop:genre <http://dbpedia.org/resource/Science_fiction>.

    FILTER langMatches( lang(?name), "EN" ).
}
GROUP BY ?author
\end{verbatim}


This is an earlier version of the grammar which was slow to parse for
various reasons mentioned in section \ref{section:sparql_gen}:
\begin{figure}[h!]
\setcounter{equation}{0}
\begin{align}
S \rightarrow\; & subject\_question \\
subject\_question \rightarrow\; & \text{WHAT} \; NP \; VP \\
subject\_question \rightarrow\; & \text{WHAT} \; VP \\
subject\_question \rightarrow\; & \text{WHO} \; VP \\
NP \rightarrow\; & Adj \; NP \\
NP \rightarrow\; & N \; \text{OR} \; N \\
NP \rightarrow\; & N \; \text{AND} \; N \\
NP \rightarrow\; & \text{BOTH} \; N \; \text{AND} \; N \\
NP \rightarrow\; & N\\
VP \rightarrow\; & \text{HAS} \; NP \; V \\
VP \rightarrow\; & V \; NP \\
Adj \rightarrow\; & \text{genre} \\
N \rightarrow\; & \text{genre} \\
N \rightarrow\; & \text{author} \\
N \rightarrow\; & \text{book} \\
N \rightarrow\; & \text{AUTHOR} \\
N \rightarrow\; & \text{BOOKS} \\
V \rightarrow\; & \text{WROTE}
\end{align}

\caption{An earlier version of the parser's grammar.}
\label{fig:slow_grammar}
\end{figure}

Finally, figure \ref{fig:output_example} is an example of the system's operation 
with full output to show each step of the process:
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth,natwidth=1,natheight=1]{imgs/demo/resolve.png}
    \caption{An example of the program's output.}
    \label{fig:output_example}
\end{figure}

\clearpage
\bibliography{thesis}{}
\bibliographystyle{plain}
\end{document}
